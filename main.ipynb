{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 关于word embedding，以序列建模为例\n",
    "# 考虑source sentence 和 target sentence（离散）\n",
    "# 构建序列，序列的字符以索引的形式表示\n",
    "batch_size = 2\n",
    "\n",
    "# 单词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "\n",
    "# 句子的最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "\n",
    "# 句子长度\n",
    "# src_len = torch.randint(2, 5, (batch_size,))  # 随机生成句子长度\n",
    "# tgt_len = torch.randint(2, 5, (batch_size,))\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)  \n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source sentence & target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5, 2, 0, 0, 0],\n",
       "         [5, 1, 1, 2, 0]]),\n",
       " tensor([[7, 3, 7, 6, 0],\n",
       "         [2, 2, 4, 0, 0]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 单词索引(token ID)构成的源句子和目标句子，构建batch，并且做了padding，默认值为0\n",
    "## 使用多行代码实现\n",
    "src_seq_list = []  # 用于存储处理后的序列\n",
    "\n",
    "for L in src_len:\n",
    "    # 生成一个形状为 (L,) 的随机整数张量，表示长度为 L 的输入序列\n",
    "    rand_seq = torch.randint(1, max_num_src_words, (L,))\n",
    "\n",
    "    # 使用 F.pad 在右侧填充 0，使序列长度变为 max_src_seq_len\n",
    "    padded_seq = F.pad(rand_seq, (0, max_src_seq_len - L))\n",
    "\n",
    "    # 在第一维（批次维度）上增加一个维度，使其变为 (1, max_src_seq_len)\n",
    "    expanded_seq = torch.unsqueeze(padded_seq, 0)\n",
    "\n",
    "    # 将处理后的序列添加到列表中\n",
    "    src_seq_list.append(expanded_seq)\n",
    "\n",
    "# 通过 torch.cat 在批次维度 (dim=0) 上拼接所有序列\n",
    "src_seq = torch.cat(src_seq_list, dim=0)  # 最终形状为 (batch_size, max_src_seq_len)\n",
    "\n",
    "## 只使用一行代码实现\n",
    "# src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len-L)), 0) for L in src_len])\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len-L)), 0) for L in tgt_len])\n",
    "\n",
    "src_seq, tgt_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2204, -0.1895,  0.1097,  0.3113, -0.2367,  1.5414,  0.5483,  0.1560],\n",
       "        [ 0.7961, -0.7196, -2.5690,  0.3586, -0.3528,  0.8761,  0.1312, -2.3167],\n",
       "        [-0.1066,  0.3241,  0.1484, -0.7014, -1.3806, -0.6581, -0.1047, -1.5250],\n",
       "        [ 1.0106, -0.6918, -1.2055,  0.9626,  1.6071,  1.1457, -0.7686,  0.1197],\n",
       "        [ 1.6573, -1.3230, -0.3683, -0.1587,  0.1823, -0.7442,  1.7799, -0.3599],\n",
       "        [ 2.1197,  1.5243,  0.1713, -0.6293,  0.8753, -0.4871, -1.0168,  1.6978],\n",
       "        [-0.1117,  0.8207,  0.5764,  0.4387,  0.7001,  1.7748, -1.8028,  1.0397],\n",
       "        [-0.6761,  0.2510, -0.4549, -1.1422, -0.9927, -1.1721,  1.1605,  0.3645],\n",
       "        [-0.9596, -0.9147,  0.7747,  1.3046, -1.1128, -0.3725, -0.9399,  0.1322]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词向量维度\n",
    "model_dim = 8\n",
    "\n",
    "# 构造embedding（调用Pytorch的embedding api）\n",
    "\n",
    "# embedding的词表\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1, model_dim)  # padding的字符占用了0这一行的索引，所以要“+1”\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim)\n",
    "src_embedding_table.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.2204, -0.1895,  0.1097,  0.3113, -0.2367,  1.5414,  0.5483,  0.1560],\n",
       "         [ 0.7961, -0.7196, -2.5690,  0.3586, -0.3528,  0.8761,  0.1312, -2.3167],\n",
       "         [-0.1066,  0.3241,  0.1484, -0.7014, -1.3806, -0.6581, -0.1047, -1.5250],\n",
       "         [ 1.0106, -0.6918, -1.2055,  0.9626,  1.6071,  1.1457, -0.7686,  0.1197],\n",
       "         [ 1.6573, -1.3230, -0.3683, -0.1587,  0.1823, -0.7442,  1.7799, -0.3599],\n",
       "         [ 2.1197,  1.5243,  0.1713, -0.6293,  0.8753, -0.4871, -1.0168,  1.6978],\n",
       "         [-0.1117,  0.8207,  0.5764,  0.4387,  0.7001,  1.7748, -1.8028,  1.0397],\n",
       "         [-0.6761,  0.2510, -0.4549, -1.1422, -0.9927, -1.1721,  1.1605,  0.3645],\n",
       "         [-0.9596, -0.9147,  0.7747,  1.3046, -1.1128, -0.3725, -0.9399,  0.1322]],\n",
       "        requires_grad=True),\n",
       " tensor([[5, 2, 0, 0, 0],\n",
       "         [5, 1, 1, 2, 0]]),\n",
       " tensor([[[ 2.1197,  1.5243,  0.1713, -0.6293,  0.8753, -0.4871, -1.0168,\n",
       "            1.6978],\n",
       "          [-0.1066,  0.3241,  0.1484, -0.7014, -1.3806, -0.6581, -0.1047,\n",
       "           -1.5250],\n",
       "          [-0.2204, -0.1895,  0.1097,  0.3113, -0.2367,  1.5414,  0.5483,\n",
       "            0.1560],\n",
       "          [-0.2204, -0.1895,  0.1097,  0.3113, -0.2367,  1.5414,  0.5483,\n",
       "            0.1560],\n",
       "          [-0.2204, -0.1895,  0.1097,  0.3113, -0.2367,  1.5414,  0.5483,\n",
       "            0.1560]],\n",
       " \n",
       "         [[ 2.1197,  1.5243,  0.1713, -0.6293,  0.8753, -0.4871, -1.0168,\n",
       "            1.6978],\n",
       "          [ 0.7961, -0.7196, -2.5690,  0.3586, -0.3528,  0.8761,  0.1312,\n",
       "           -2.3167],\n",
       "          [ 0.7961, -0.7196, -2.5690,  0.3586, -0.3528,  0.8761,  0.1312,\n",
       "           -2.3167],\n",
       "          [-0.1066,  0.3241,  0.1484, -0.7014, -1.3806, -0.6581, -0.1047,\n",
       "           -1.5250],\n",
       "          [-0.2204, -0.1895,  0.1097,  0.3113, -0.2367,  1.5414,  0.5483,\n",
       "            0.1560]]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "\n",
    "# src\n",
    "src_embedding_table.weight, src_seq, src_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-1.3400,  0.0219,  1.1850,  0.1048,  1.1570, -1.3731, -1.1395,  0.1921],\n",
       "         [-1.0655, -1.2237, -1.2906, -0.2561, -0.6014,  0.5297, -0.2648,  0.1721],\n",
       "         [-0.0311,  0.8696, -0.0472, -1.0195, -1.0602,  0.2994, -0.5326, -0.7094],\n",
       "         [-0.8571,  1.7570,  0.8558,  0.3231, -0.3227, -2.3728,  0.0254,  0.2394],\n",
       "         [-1.1436, -0.5389,  0.0222, -1.5995,  0.2235,  0.4767,  0.1759,  0.9656],\n",
       "         [ 0.8851,  0.4446,  1.0952,  0.5664,  0.3982,  1.9940,  1.9620,  2.0426],\n",
       "         [-0.6210,  0.9583, -0.7212, -0.4852,  1.8842, -0.0626, -1.4300, -0.4557],\n",
       "         [-0.1287,  0.1977,  0.2854,  1.2646,  1.5819,  0.3283, -3.1076, -1.4709],\n",
       "         [-0.2448, -0.3207, -0.6010,  0.1534,  0.0171, -0.8729, -1.1606,  0.4831]],\n",
       "        requires_grad=True),\n",
       " tensor([[7, 3, 7, 6, 0],\n",
       "         [2, 2, 4, 0, 0]]),\n",
       " tensor([[[-0.1287,  0.1977,  0.2854,  1.2646,  1.5819,  0.3283, -3.1076,\n",
       "           -1.4709],\n",
       "          [-0.8571,  1.7570,  0.8558,  0.3231, -0.3227, -2.3728,  0.0254,\n",
       "            0.2394],\n",
       "          [-0.1287,  0.1977,  0.2854,  1.2646,  1.5819,  0.3283, -3.1076,\n",
       "           -1.4709],\n",
       "          [-0.6210,  0.9583, -0.7212, -0.4852,  1.8842, -0.0626, -1.4300,\n",
       "           -0.4557],\n",
       "          [-1.3400,  0.0219,  1.1850,  0.1048,  1.1570, -1.3731, -1.1395,\n",
       "            0.1921]],\n",
       " \n",
       "         [[-0.0311,  0.8696, -0.0472, -1.0195, -1.0602,  0.2994, -0.5326,\n",
       "           -0.7094],\n",
       "          [-0.0311,  0.8696, -0.0472, -1.0195, -1.0602,  0.2994, -0.5326,\n",
       "           -0.7094],\n",
       "          [-1.1436, -0.5389,  0.0222, -1.5995,  0.2235,  0.4767,  0.1759,\n",
       "            0.9656],\n",
       "          [-1.3400,  0.0219,  1.1850,  0.1048,  1.1570, -1.3731, -1.1395,\n",
       "            0.1921],\n",
       "          [-1.3400,  0.0219,  1.1850,  0.1048,  1.1570, -1.3731, -1.1395,\n",
       "            0.1921]]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_embedding_table.weight, tgt_seq, tgt_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造position embedding\n",
    "\n",
    "# 定义训练编码的最大长度\n",
    "max_postion_len = 5\n",
    "\n",
    "# 在position embedding的公式中：pos表示行，i表示列\n",
    "# 目标：构造出pos矩阵（每行数字相同）和i矩阵（每列数字相同）\n",
    "pos_mat = torch.arange(max_postion_len).reshape((-1, 1))\n",
    "pos_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2500, 0.5000, 0.7500]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_mat = torch.arange(0, 8, 2).reshape((1, -1)) / model_dim\n",
    "i_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1.,   10.,  100., 1000.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_mat = torch.pow(10000, torch.arange(0, 8, 2).reshape((1, -1)) / model_dim)\n",
    "i_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "          9.9920e-01,  4.0000e-03,  9.9999e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_embedding_table = torch.zeros(max_postion_len, model_dim)\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat)\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)\n",
    "pe_embedding_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "           9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "         [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "           9.9920e-01,  4.0000e-03,  9.9999e-01]]),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "           9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "         [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "           9.9920e-01,  4.0000e-03,  9.9999e-01]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_embedding = nn.Embedding(max_postion_len, model_dim)\n",
    "\n",
    "# 将pe_embedding_table赋值给pe_embedding.weight，并设置requires_grad为False，表示pe_embedding.weight的值不允许被梯度更新\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)  \n",
    "pe_embedding_table, pe_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3])],\n",
       " [tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3])])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到源序列的位置索引(注意：这里不能使用单词索引)\n",
    "src_pos = [torch.arange(max(src_len)) for _ in src_len]\n",
    "tgt_pos = [torch.arange(max(tgt_len)) for _ in tgt_len]\n",
    "src_pos, tgt_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "            9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "            9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "          [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "            9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "            9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "            9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "          [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "            9.9955e-01,  3.0000e-03,  1.0000e+00]]]),\n",
       " tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "            9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "            9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "          [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "            9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "            9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "            9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "          [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "            9.9955e-01,  3.0000e-03,  1.0000e+00]]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)), 0) for _ in src_len]).to(torch.int32)\n",
    "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)), 0) for _ in tgt_len]).to(torch.int32)\n",
    "\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "src_pe_embedding, tgt_pe_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder self-attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1929, 0.2202, 0.1919, 0.1891, 0.2059]),\n",
       " tensor([1.7575e-06, 9.9876e-01, 1.0515e-06, 2.4779e-07, 1.2404e-03]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax演示: scaled的重要性\n",
    "alpha1, alpha2 = 0.1, 10\n",
    "score = torch.randn(5)  # score表示 Q * K 的结果\n",
    "prob1 = F.softmax(score*alpha1, dim=-1)\n",
    "prob2 = F.softmax(score*alpha2, dim=-1)\n",
    "prob1, prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5p/0xkd6yn139x0pbr3v6c67hsm0000gn/T/ipykernel_63800/603754109.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(score)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1557, -0.0425, -0.0370, -0.0365, -0.0397],\n",
       "         [-0.0425,  0.1717, -0.0422, -0.0416, -0.0453],\n",
       "         [-0.0370, -0.0422,  0.1551, -0.0363, -0.0395],\n",
       "         [-0.0365, -0.0416, -0.0363,  0.1534, -0.0389],\n",
       "         [-0.0397, -0.0453, -0.0395, -0.0389,  0.1635]]),\n",
       " tensor([[ 1.7575e-06, -1.7553e-06, -1.8481e-12, -4.3548e-13, -2.1801e-09],\n",
       "         [-1.7553e-06,  1.2419e-03, -1.0502e-06, -2.4748e-07, -1.2389e-03],\n",
       "         [-1.8481e-12, -1.0502e-06,  1.0515e-06, -2.6056e-13, -1.3044e-09],\n",
       "         [-4.3548e-13, -2.4748e-07, -2.6056e-13,  2.4779e-07, -3.0736e-10],\n",
       "         [-2.1801e-09, -1.2389e-03, -1.3044e-09, -3.0736e-10,  1.2389e-03]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_func(score):\n",
    "    return F.softmax(score)\n",
    "\n",
    "# jacobian\n",
    "jaco_mat1 = torch.autograd.functional.jacobian(softmax_func, score*alpha1)\n",
    "jaco_mat2 = torch.autograd.functional.jacobian(softmax_func, score*alpha2)\n",
    "jaco_mat1, jaco_mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1.]), tensor([1., 1., 1., 1.])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造encoder self-attention mask\n",
    "# mask的shape：[batch_size, max_src_len, max_src_len]，值为1或-inf\n",
    "valid_encoder_pos = [torch.ones(L) for L in src_len]\n",
    "valid_encoder_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 0., 0.]), tensor([1., 1., 1., 1.])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding\n",
    "valid_encoder_pos = [F.pad(torch.ones(L), (0, max(src_len)-L)) for L in src_len]\n",
    "valid_encoder_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1., 1., 0., 0.]]), tensor([[1., 1., 1., 1.]])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 扩维（0 -> 1）\n",
    "valid_encoder_pos = [torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len)-L)), 0) for L in src_len]\n",
    "valid_encoder_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 继续扩维\n",
    "valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len)-L)), 0) \\\n",
    "                                               for L in src_len]), 2)\n",
    "valid_encoder_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 4]),\n",
       " tensor([[[1., 1., 0., 0.],\n",
       "          [1., 1., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "valid_encoder_pos_matrix.shape, valid_encoder_pos_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 1., 1.],\n",
       "         [0., 0., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid_encoder_pos_matrix = 1 - valid_encoder_pos_matrix\n",
    "invalid_encoder_pos_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [ True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False, False],\n",
       "         [False, False, False, False],\n",
       "         [False, False, False, False],\n",
       "         [False, False, False, False]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True表示需要mask，False表示不需要mask\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "mask_encoder_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4]) torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "score = torch.randn(batch_size, max(src_len), max(src_len))\n",
    "print(score.shape, mask_encoder_self_attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 4], dtype=torch.int32),\n",
       " tensor([[[ 0.0997,  1.0616,  1.7244, -0.0042],\n",
       "          [-0.4897,  1.4075, -0.1138,  0.0997],\n",
       "          [ 0.1757,  1.2486, -0.6869, -0.2622],\n",
       "          [-0.2621,  1.4763,  1.1949,  0.5869]],\n",
       " \n",
       "         [[ 1.5380, -1.2645,  0.3520,  0.4939],\n",
       "          [-0.4976, -0.1894,  0.4387,  0.3130],\n",
       "          [-1.3573,  0.2773,  1.2513,  1.7238],\n",
       "          [ 0.7528, -0.0915,  0.0259,  2.3903]]]),\n",
       " tensor([[[ 9.9718e-02,  1.0616e+00, -1.0000e+09, -1.0000e+09],\n",
       "          [-4.8972e-01,  1.4075e+00, -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 1.5380e+00, -1.2645e+00,  3.5200e-01,  4.9389e-01],\n",
       "          [-4.9756e-01, -1.8943e-01,  4.3875e-01,  3.1299e-01],\n",
       "          [-1.3573e+00,  2.7730e-01,  1.2513e+00,  1.7238e+00],\n",
       "          [ 7.5282e-01, -9.1533e-02,  2.5895e-02,  2.3903e+00]]]),\n",
       " tensor([[[0.2765, 0.7235, 0.0000, 0.0000],\n",
       "          [0.1304, 0.8696, 0.0000, 0.0000],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "         [[0.5820, 0.0353, 0.1778, 0.2049],\n",
       "          [0.1397, 0.1901, 0.3562, 0.3141],\n",
       "          [0.0241, 0.1236, 0.3273, 0.5250],\n",
       "          [0.1417, 0.0609, 0.0685, 0.7288]]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
    "prob = F.softmax(masked_score, dim=-1)\n",
    "src_len, score, masked_score, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intra-attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder self-attetion mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
